{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg as linalg\n",
    "from scipy.sparse import csc_matrix, bmat, load_npz, csr_matrix\n",
    "from scipy.sparse.linalg import svds, eigsh\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import implicit\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = load_npz(\"data/yt_s.npz\")\n",
    "a_train = load_npz(\"data/yt_a_train.npz\")\n",
    "a_test = load_npz(\"data/yt_a_test.npz\")\n",
    "a_val = load_npz(\"data/yt_a_val.npz\")\n",
    "n_groups = a_train.shape[1]\n",
    "n_users = s.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(alpha, s, a_train, a_val):\n",
    "    c_train = bmat([[alpha*s, a_train], [a_train.transpose(), None]])\n",
    "    c_val = bmat([[alpha*s, a_val], [a_val.transpose(), None]])\n",
    "    #c_test = bmat([[alpha*s, a_test], [a_test.transpose(), None]])    \n",
    "    return c_train.astype(np.float64), c_val.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_get_recs(i, model, train_labels, n_groups):\n",
    "        u, sig, vt = model\n",
    "        score = (u[i,:]@np.diag(sig)@vt)[-n_groups:]\n",
    "        score = np.multiply(score, np.logical_not(train_labels))\n",
    "        score_index = np.flip(np.argsort(score))\n",
    "        return score_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"vrati listu (za k = 1:n) precisiona i recalla na testu za jednog usera\"\n",
    "def evaluate_model_user(i, n, model, c_train, c_val, n_groups, model_type):\n",
    "    \"i = user za kojeg generiramo recommendatione\"\n",
    "    true_labels = c_val.getrow(i).toarray().flatten()[-n_groups:]\n",
    "    train_labels = c_train.getrow(i).toarray().flatten()[-n_groups:]\n",
    "    \n",
    "    \"tu se dodaju novi modeli\"\n",
    "    if model_type == \"SVD\":\n",
    "        score_index = svd_get_recs(i, model, train_labels, n_groups)\n",
    "    elif model_type == \"ALS\":\n",
    "        score_index = als_get_recs(i, n, model, c_train, n_groups)\n",
    "    elif model_type == \"random_katz\":\n",
    "        score_index = rand_katz_get_recs(i, n, model, train_labels, n_groups)\n",
    "    elif model_type == \"perfect_model\":\n",
    "        score_index = np.flip(np.argsort(true_labels))\n",
    "    elif model_type == \"katz\":\n",
    "        score_index = katz_get_recs(i,model, train_labels, n_groups)\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    positives = np.sum(true_labels)\n",
    "    negatives = n_groups - positives\n",
    "    user_i_stats = []\n",
    "    for predictions in range(1, n+1):\n",
    "        recommendations = score_index[:predictions]\n",
    "        true_positives = np.sum(true_labels[recommendations] == 1)\n",
    "        true_negatives = negatives - (predictions - true_positives)\n",
    "        precision = true_positives/predictions\n",
    "        sensitivity = true_positives/positives\n",
    "        specificity = true_negatives/negatives\n",
    "        user_i_stats.append((precision, sensitivity, specificity))\n",
    "    return user_i_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, c_train, c_val, n_users, n_groups, model_type):\n",
    "    stats = []\n",
    "    for i in tqdm(range(n_users)):\n",
    "            if np.sum(c_val.getrow(i).toarray().flatten()[-n_groups:]) != 0:\n",
    "                stats.append(evaluate_model_user(i, 50, model, c_train, c_val, n_groups, model_type))\n",
    "    \"stats mi je lista duljine broj usera, svaki element je lista duljine n koja sadrzi tupleove oblika (pr, se, sp)\"\n",
    "    pr_se_sp = []\n",
    "    \"pr_se_sp ce biti lista tupleova duljine n, tuple je oblika (mean_pr, mean_se, mean_sp) gdje je prosjek uzet po userima\"\n",
    "    for n in zip(*stats):\n",
    "        pr_se_sp.append((np.mean([i for i,j,k in n]), np.mean([j for i,j,k in n]), np.mean([k for i,j,k in n])))\n",
    "    \"pss ce biti numpy array dimenzija n x 3, svaki stupac odgovara jednom od (pr, se, sp)\"\n",
    "    pss = np.array(pr_se_sp)\n",
    "    return pss[:,0], pss[:,1], pss[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(precision, sensitivity, specificity):\n",
    "    x=[(1-spec) for spec in specificity]\n",
    "    area = np.trapz(y=sensitivity, x=x)\n",
    "    return abs(area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_model(alpha, svd_rank, s, a_train, a_val, n_users, n_groups):\n",
    "    c_train, c_val = prepare_train_data(alpha, s, a_train, a_val)\n",
    "    model = svds(c_train, k = svd_rank)\n",
    "    precision, sensitivity, specificity = evaluate_model(model, c_train, c_val, n_users, n_groups, model_type = \"SVD\")\n",
    "    score = get_score(precision, sensitivity, specificity)\n",
    "    return {\"alpha\" : alpha, \"svd_rank\" : svd_rank, \"score\" : score, \"precision\" : precision, \"sensitivity\" : sensitivity, \"specificity\" : specificity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_svd_model(alphas, svd_ranks, s, a_train, a_val, n_users, n_groups):\n",
    "    validation_scores = []\n",
    "    for alpha in alphas:\n",
    "        for svd_rank in svd_ranks:\n",
    "            validation_scores.append(svd_model(alpha, svd_rank, s, a_train, a_val, n_users, n_groups))\n",
    "    return validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [2.5]\n",
    "svd_ranks = [700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_svd = validate_svd_model(alphas, svd_ranks, s, a_train, a_val, n_users, n_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_svd.sort(key = lambda x : x[\"score\"], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_svd[0][\"alpha\"], validation_scores_svd[0][\"svd_rank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_svd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in validation_scores_svd:\n",
    "    print(score[\"alpha\"], score[\"svd_rank\"], score[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUACIJA GENERALNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"treba u evaluate_model_user za svaki model dodati granu u if-u u funkciji evaluate_model_user u kojoj se napravi score_index\"\n",
    "\"score_index je lista/np.array koji sadrži indekse grupa sortirane po scoreu koji model daje, dakle sortirana lista grupa za recommendat\"\n",
    "\"ideja je da se dotad sve sto ti treba za evaluirati model prenosi u varijabli model, a onda unutar tog ifa se pozove neka funkcija koja evaluira\"\n",
    "\"za validaciju i kreiranje modela predlazem da se rade posebne funkcije za svaki jer nije bas zgodno napravit generalno, mogu biti po uzoru na ove\"\n",
    "#precision, sensitivity, specificity = evaluate_model(model, c_train, c_val, n_users, n_groups, model_type = \"SVD\")\n",
    "#score = get_score(precision, sensitivity, specificity)\n",
    "\"dole primjer za als - 3 modificirane funkcije i onda se samo pozove - nije bas savrseno al mislim da ce bit skroz ok za nasih par modela\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_get_recs(i, n, model, c_train, n_groups):\n",
    "    recs = model.recommend(i, user_items = csr_matrix(c_train), N = n, filter_already_liked_items = True, filter_items = [j for j in range(c_train.shape[0]-n_groups)])\n",
    "    indices = [rec[0]-n_users for rec in recs]\n",
    "    scores = [rec[1] for rec in recs]\n",
    "    score = np.zeros(n_groups)\n",
    "    score[indices] = scores\n",
    "    score_index = np.flip(np.argsort(score))\n",
    "    return score_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_model(alpha, n_factors, s, a_train, a_val, n_users, n_groups):\n",
    "    c_train, c_val = prepare_train_data(alpha, s, a_train, a_val)\n",
    "    model = implicit.als.AlternatingLeastSquares(factors = n_factors, regularization = 2)\n",
    "    model.fit(c_train)    \n",
    "    precision, sensitivity, specificity = evaluate_model(model, c_train, c_val, n_users, n_groups, model_type = \"ALS\")\n",
    "    score = get_score(precision, sensitivity, specificity)\n",
    "    return {\"alpha\" : alpha, \"n_factors\" : n_factors, \"score\" : score, \"precision\" : precision, \"sensitivity\" : sensitivity, \"specificity\" : specificity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_als_model(alphas, n_factors, s, a_train, a_val, n_users, n_groups):\n",
    "    validation_scores = []\n",
    "    for alpha in alphas:\n",
    "        for nf in n_factors:\n",
    "            validation_scores.append(als_model(alpha, nf, s, a_train, a_val, n_users, n_groups))\n",
    "    return validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [7]\n",
    "n_factors = [600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_als = validate_als_model(alphas, n_factors, s, a_train, a_val, n_users, n_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_als.sort(key = lambda x : x[\"score\"], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_als[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_als[0][\"alpha\"], validation_scores_als[0][\"n_factors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in validation_scores_als:\n",
    "    print(score[\"alpha\"], score[\"n_factors\"], score[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Katz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def katz (A, S, k, rank, beta, alfa):\n",
    "    S = S.astype(float)\n",
    "    vals, vecs = eigsh(S, k=rank)\n",
    "    matrix = A.astype(float)\n",
    "    U, s, V = svds( matrix, k=rank)\n",
    "    common_space = np.hstack([vecs,U])\n",
    "    q, r = linalg.qr(common_space, mode=\"economic\")\n",
    "    Ds = q.transpose()@S@ q\n",
    "    V, r1 = linalg.qr(A.transpose()@q,mode=\"economic\")\n",
    "    Vt=V.transpose()\n",
    "    Da = q.transpose()@ A @ V\n",
    "    x1 = alfa*Ds@Da\n",
    "    last_factors = [[x1,1]]\n",
    "    # 1 ce mi biti indikator jel prvi sumand matrica Ds ili nije\n",
    "    if k==1:\n",
    "        return beta*q@Da@Vt\n",
    "    beta=beta**2\n",
    "    final_sum = beta*x1\n",
    "    if k==2:\n",
    "        return prvi_sumand+q@final_sum@Vt\n",
    "    for i in range(k-2):\n",
    "        novi = []\n",
    "        for fact in last_factors:\n",
    "            if fact[1]==1:\n",
    "                novi.append([alfa*Ds@fact[0],1])\n",
    "                novi.append([((1/alfa)*Da@Da.transpose()@np.linalg.inv(Ds)@fact[0]),0])\n",
    "            if fact[1]==0:\n",
    "                novi.append([alfa*Ds@fact[0],1])\n",
    "        ita_suma = sum(matr[0] for matr in novi)\n",
    "        beta=beta*beta\n",
    "        final_sum = final_sum + beta*ita_suma\n",
    "        last_factors = novi\n",
    "    #rez = beta*q@Ds@q.transpose()+q@final_sum@Vt\n",
    "    \n",
    "    #rez = q@final_sum@Vt\n",
    "    final_sum = beta*Da+final_sum\n",
    "    return q,final_sum, Vt\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def katz_get_recs(i, model, train_labels, n_groups):\n",
    "        q, final_sum, vt = model\n",
    "        score = (q[i,:]@final_sum@vt)\n",
    "        score = np.multiply(score, np.logical_not(train_labels))\n",
    "        score_index = np.flip(np.argsort(score))\n",
    "        return score_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def katz_model(alpha,beta, katz_rank, s, a_train, a_val, n_users, n_groups, k):\n",
    "    #katz (A, S, k, d, beta, alfa):\n",
    "    #c_train, c_val = prepare_train_data(alpha, s, a_train, a_val)\n",
    "    model = katz(a_train, s,k ,katz_rank, beta, alpha)\n",
    "    precision, sensitivity, specificity = evaluate_model(model, a_train, a_val, n_users, n_groups, model_type = \"katz\")\n",
    "    score = get_score(precision, sensitivity, specificity)\n",
    "    return {\"beta\" : beta, \"alpha\" : alpha, \"katz_rank\" : katz_rank, \"score\" : score, \"precision\" : precision, \"sensitivity\" : sensitivity, \"specificity\" : specificity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_katz_model(alphas, betas, katz_ranks, s, a_train, a_val, n_users, n_groups, k):\n",
    "    validation_scores = []\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            for katz_rank in katz_ranks:\n",
    "                validation_scores.append(katz_model(alpha, beta, katz_rank, s, a_train, a_val, n_users, n_groups, k))\n",
    "    return validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1]\n",
    "betas = [0.1]\n",
    "katz_ranks = [600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_katz = validate_katz_model(alphas,betas, katz_ranks, s, a_train, a_val, n_users, n_groups, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_katz.sort(key = lambda x : x[\"score\"], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_katz[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores_katz[0][\"alpha\"], validation_scores_katz[0][\"beta\"], validation_scores_katz[0][\"katz_rank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in validation_scores_katz:\n",
    "    print(score[\"alpha\"], score[\"beta\"],score[\"katz_rank\"], score[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Katz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_katz_get_recs(i, n, model, train_labels, n_groups):\n",
    "    recs = model[i+1] # i is iterating from 0 to n_users, user_id in dict is between 1 and n_users\n",
    "    score = np.multiply(recs, np.logical_not(train_labels))\n",
    "    score_index = np.flip(np.argsort(score))\n",
    "    return score_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def read_results_from_bin(bin_file_path, n_groups):\n",
    "    # format is uint32, uint32, double\n",
    "    fmt = \"IId\"\n",
    "    record_len = struct.calcsize(fmt)\n",
    "    unpack = struct.Struct(fmt).unpack_from\n",
    "    result = {}\n",
    "    with open(bin_file_path, \"rb\") as file:\n",
    "        data = file.read(record_len)\n",
    "        while data:\n",
    "            user, group, score = unpack(data)\n",
    "            if user not in result:\n",
    "                result[user] = np.zeros(n_groups)\n",
    "            result[user][group-1] = score\n",
    "            data = file.read(record_len)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_katz_model(file, s, a_train, a_val, n_users, n_groups):\n",
    "    model = read_results_from_bin(str(file), n_groups)   #model je izracunat vec u nekom c++ kodu, ovdje samo učitaom file\n",
    "    c_train, c_val = prepare_train_data(1, s, a_train, a_val)\n",
    "    precision, sensitivity, specificity = evaluate_model(model, c_train, c_val, n_users, n_groups, model_type = \"random_katz\")\n",
    "    score = get_score(precision, sensitivity, specificity)\n",
    "    path_len, iterations = tuple(int(s) for s in re.findall(r'\\d+', str(file)))       # cisto da imamo podatke o duljini puta i broju iteracija u ovom modelu\n",
    "    return {\"iterations\": iterations, \"path_len\": path_len, \"score\": score, \"precision\": precision, \"sensitivity\": sensitivity, \"specificity\": specificity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_random_katz_model(s, a_train, a_val, n_users, n_groups):\n",
    "    validation_scores = []\n",
    "    for file in Path(\"data/random_katz/results/\").iterdir():\n",
    "        path_len, iterations = tuple(int(s) for s in re.findall(r'\\d+', str(file)))\n",
    "        if path_len == 5 and iterations == 10000:    #ovo je ispalo najbolje\n",
    "            validation_scores.append(random_katz_model(file, s, a_train, a_val, n_users, n_groups))\n",
    "    return validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rand_katz_validation_scores = validate_random_katz_model(s, a_train, a_val, n_users, n_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_katz_validation_scores.sort(key= lambda x : x[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_score in rand_katz_validation_scores:\n",
    "    plt.plot(1 - val_score[\"specificity\"], val_score[\"sensitivity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualizacija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "sns.lineplot(x = 1 - validation_scores_als[0][\"specificity\"], y = validation_scores_als[0][\"sensitivity\"])\n",
    "sns.lineplot(x = 1 - validation_scores_svd[0][\"specificity\"], y = validation_scores_svd[0][\"sensitivity\"])\n",
    "sns.lineplot(x = 1 - validation_scores_katz[0][\"specificity\"], y = validation_scores_katz[0][\"sensitivity\"])\n",
    "sns.lineplot(x = 1 - rand_katz_validation_scores[0][\"specificity\"], y = rand_katz_validation_scores[0][\"sensitivity\"])\n",
    "plt.legend((\"ALS\", \"SVD\", \"KATZ\", \"RAND_KATZ\"))\n",
    "plt.xlabel(\"1 - specificity\")\n",
    "plt.ylabel(\"sensitivity\")\n",
    "plt.title(\"ROC CURVES FOR ALL MODELS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
